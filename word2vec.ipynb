{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.0033901 -0.34614    0.28144    0.48382    0.59469    0.012965\n",
            "  0.53982    0.48233    0.21463   -1.0249    -0.34788   -0.79001\n",
            " -0.15084    0.61374    0.042811   0.19323    0.25462    0.32528\n",
            "  0.05698    0.063253  -0.49439    0.47337   -0.16761    0.045594\n",
            "  0.30451   -0.35416   -0.34583   -0.20118    0.25511    0.091111\n",
            "  0.014651  -0.017541  -0.23854    0.48215   -0.9145    -0.36235\n",
            "  0.34736    0.028639  -0.027065  -0.036481  -0.067391  -0.23452\n",
            " -0.13772    0.33951    0.13415   -0.1342     0.47856   -0.1842\n",
            "  0.10705   -0.45834   -0.36085   -0.22595    0.32881   -0.13643\n",
            "  0.23128    0.34269    0.42344    0.47057    0.479      0.074639\n",
            "  0.3344     0.10714   -0.13289    0.58734    0.38616   -0.52238\n",
            " -0.22028   -0.072322   0.32269    0.44226   -0.037382   0.18324\n",
            "  0.058082   0.26938    0.36202    0.13983    0.016815  -0.34426\n",
            "  0.4827     0.2108     0.75618   -0.13092   -0.025741   0.43391\n",
            "  0.33893   -0.16438    0.26817    0.68774    0.311     -0.2509\n",
            "  0.0027749 -0.39809   -0.43399    0.049531  -0.42686   -0.094679\n",
            "  0.56925    0.28742   -0.015721  -0.059162   0.1912    -0.59814\n",
            "  0.65486   -0.31363    0.16881    0.10862    0.075316   0.34093\n",
            " -0.14706    0.8359     0.39697    0.52358   -0.0096367 -0.14406\n",
            "  0.37783   -0.596     -0.063192  -0.85297   -0.3098    -1.0587\n",
            " -1.025      0.4508    -0.73324   -1.2461    -0.028488   0.20299\n",
            "  0.00259    0.31995    0.35744    0.28533    0.228      0.50956\n",
            " -0.35942    0.32683    0.046264  -0.86896   -0.2707    -0.15454\n",
            " -0.32152    0.31121    0.44134    0.85189    0.21065   -0.13741\n",
            " -0.15359   -0.059722   0.027375   0.23724   -0.39197   -0.66065\n",
            "  0.23587    0.032384  -0.64043    0.55004    0.29597    0.14989\n",
            "  0.46079   -0.26561   -0.1607    -0.36328    1.0782     0.31375\n",
            "  0.1149     0.20248    0.032748   0.41082   -0.082536   0.36606\n",
            "  0.18771    0.75415    0.079648   0.24181   -0.60319   -0.37296\n",
            " -0.047767   0.45008   -0.21135    0.022251  -0.084325   0.18644\n",
            " -0.14682    0.56571   -0.30995    0.17423   -0.41122   -0.84772\n",
            " -0.71114    0.69895   -0.13008   -0.34195   -0.30501   -0.12646\n",
            "  0.29957   -0.43488    0.31935    0.2817    -0.20631   -0.48877\n",
            "  0.34477    0.03907    1.6198    -0.6352    -0.0037675 -0.41271\n",
            "  0.30704   -0.50486    0.036385  -0.046386  -0.12004    0.010029\n",
            " -0.49116    0.041486   0.002979  -0.57694   -0.42088   -0.063218\n",
            "  0.0034244 -0.25093   -0.39689   -0.36984    0.32689    0.01385\n",
            "  0.23634   -0.055199  -0.58453    0.13211    0.50943    0.25198\n",
            " -0.0088309 -0.21273   -0.48423    0.5234    -0.32832   -0.013821\n",
            "  0.15812    0.46696    0.036822  -0.090878   0.18854    0.20794\n",
            " -0.42682    0.59705    0.53109    0.19185   -0.16392    0.064956\n",
            " -0.36009   -0.59882   -0.28134    0.1017     0.02601    0.44298\n",
            " -0.31922   -0.22432    0.7828     0.041307   0.1742     0.27777\n",
            "  0.43792   -0.84324    0.27012   -0.21547    0.52408   -0.19426\n",
            " -0.21878   -0.20713    0.092994  -0.15804    0.28716   -0.11911\n",
            " -0.20688   -0.36482    0.68548   -0.10394   -0.49974   -0.47038\n",
            " -1.2953    -0.46236    0.44467    0.13337    0.88762   -0.26494\n",
            "  0.080676  -0.20625   -0.51232    0.31112    0.062035   0.30302\n",
            " -0.33344   -0.20924   -0.17348   -0.43434   -0.45743   -0.077803\n",
            " -0.33248   -0.078633   0.82182    0.082088  -0.68795    0.30266  ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]  \n",
        "            vector = np.asarray(values[1:], dtype='float32') \n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_path = 'glove.6B/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "print(glove_embeddings.get('king'))  # Replace 'king' with any word to test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_simlex(filepath):\n",
        "    simlex_data = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        reader = csv.reader(file, delimiter='\\t')\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            word1, word2, similarity = row[0], row[1], float(row[3])\n",
        "            simlex_data.append((word1, word2, similarity))\n",
        "    return simlex_data\n",
        "simlex_path = 'SimLex-999/SimLex-999/SimLex-999.txt'  # Replace with your file path\n",
        "simlex_data = load_simlex(simlex_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Convert lists to numpy arrays for easier vector operations\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    \n",
        "    # Compute the dot product of the two vectors\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    \n",
        "    # Compute the magnitude (norm) of each vector\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    \n",
        "    # Compute and return the cosine similarity\n",
        "    if norm_vec1 > 0 and norm_vec2 > 0:\n",
        "        return dot_product / (norm_vec1 * norm_vec2)\n",
        "    else:\n",
        "        return None  # Avoid division by zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 word pairs with highest similarity:\n",
            "wife - husband: 0.8646\n",
            "movie - film: 0.8589\n",
            "son - father: 0.8563\n",
            "brother - son: 0.8323\n",
            "father - brother: 0.8206\n",
            "Spearman Correlation: 0.3707401409425457\n"
          ]
        }
      ],
      "source": [
        "def spearman_rank_correlation(x, y):\n",
        "    # Step 1: Rank the values\n",
        "    rank_x = {val: rank for rank, val in enumerate(sorted(x), start=1)}\n",
        "    rank_y = {val: rank for rank, val in enumerate(sorted(y), start=1)}\n",
        "    \n",
        "    # Step 2: Compute the rank differences (d_i) and d_i^2\n",
        "    rank_diff = [(rank_x[val1] - rank_y[val2])**2 for val1, val2 in zip(x, y)]\n",
        "    \n",
        "    # Step 3: Calculate the Spearman's rank correlation coefficient\n",
        "    n = len(x)\n",
        "    rho = 1 - (6 * sum(rank_diff)) / (n * (n**2 - 1))\n",
        "    return rho\n",
        "\n",
        "def evaluate_embeddings(simlex_data, embeddings):\n",
        "    predicted_similarities = []\n",
        "    human_similarities = []\n",
        "    similarity_scores = []\n",
        "    word_pairs = []\n",
        "    \n",
        "    for word1, word2, human_score in simlex_data:\n",
        "        vec1 = embeddings.get(word1)\n",
        "        vec2 = embeddings.get(word2)\n",
        "        similarity = cosine_similarity(vec1, vec2)\n",
        "        \n",
        "        if similarity is not None:  \n",
        "            predicted_similarities.append(similarity)\n",
        "            human_similarities.append(human_score)\n",
        "            similarity_scores.append(similarity)\n",
        "            word_pairs.append((word1, word2))\n",
        "        top_5_pairs = sorted(zip(similarity_scores, word_pairs), reverse=True)[:5]\n",
        "    \n",
        "    print(\"Top 5 word pairs with highest similarity:\")\n",
        "    for similarity, (word1, word2) in top_5_pairs:\n",
        "        print(f\"{word1} - {word2}: {similarity:.4f}\")\n",
        "    \n",
        "    spearman_corr = spearman_rank_correlation(predicted_similarities, human_similarities)\n",
        "    return spearman_corr\n",
        "spearman_corr = evaluate_embeddings(simlex_data, glove_embeddings)\n",
        "print(f\"Spearman Correlation: {spearman_corr}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
